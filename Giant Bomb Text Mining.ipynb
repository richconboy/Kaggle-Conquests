{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Giant Bomb Text Mining\n",
    "In this notebook, we are going to access the Giant Bomb API via Python and use game descriptions to try and identify common genres and classify games accordingly using clustering techniques.  First, let's import our tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import nltk \n",
    "import re\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we're going to define a function to pull down our game descriptions from the Giant Bomb API.  Note that the Giant Bomb API only lets us pull down 50 games at a time so we need to use the \"**offset**\" argument to iterate through the whole library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define a function that returns a JSON object from the Giant Bomb API\n",
    "def pull_info(resource_type, resource_id='', offset=0):\n",
    "    bomb = 'http://www.giantbomb.com/api/'\n",
    "    resource_type = resource_type + '/'\n",
    "    if resource_id != \"\":\n",
    "        resource_id = resource_id + '/'\n",
    "    api_key = '?api_key=98fade4b69e9695cee10f3d8a8f9cb69a5d03c55'\n",
    "    json = '&format=json'\n",
    "    offset = '&offset=' + str(offset)\n",
    "    \n",
    "    bomb += resource_type\n",
    "    bomb += resource_id\n",
    "    bomb += api_key\n",
    "    bomb += json\n",
    "    bomb += offset\n",
    "    \n",
    "    bomb_output = requests.get(bomb)\n",
    "    bomb_json = bomb_output.json()\n",
    "    \n",
    "    return bomb_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create another function to actually create our dictionary of game information, using the \"**pull_info**\" function we defined above.  Then we call the function and create our dictionary, \"**game_data**.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Define a function that uses our JSON object to create our data\n",
    "def create_full_db(resource_type, resource_id='', offset=0):\n",
    "    game_dict = {'names': [], 'decks': [], 'platforms' : [], 'releases' : []}\n",
    "    bomb_length = pull_info(resource_type, resource_id, 0)\n",
    "    pages = (bomb_length['number_of_total_results']/100) + 1\n",
    "    for page in range(pages):\n",
    "        bomb_data = pull_info(resource_type, resource_id, page*100)        \n",
    "        for game in bomb_data['results']:\n",
    "            if game['deck'] != None:\n",
    "                game_dict['names'].append(game['name'])\n",
    "                game_dict['decks'].append(game['deck'])\n",
    "                game_dict['platforms'].append(game['platforms'])\n",
    "                game_dict['releases'].append(game['original_release_date'])\n",
    "    return game_dict\n",
    "game_data = create_full_db('games')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a quick look at what our dictionary looks like by reading it into a Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>decks</th>\n",
       "      <th>names</th>\n",
       "      <th>platforms</th>\n",
       "      <th>releases</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A top-down isometric helicopter shoot 'em up o...</td>\n",
       "      <td>Desert Strike: Return to the Gulf</td>\n",
       "      <td>[{u'api_detail_url': u'http://www.giantbomb.co...</td>\n",
       "      <td>1992-02-29 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Breakfree is a block-breaking game that is sim...</td>\n",
       "      <td>Breakfree</td>\n",
       "      <td>[{u'api_detail_url': u'http://www.giantbomb.co...</td>\n",
       "      <td>1995-12-31 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Chessmaster 2000 is the chess game that be...</td>\n",
       "      <td>The Chessmaster 2000</td>\n",
       "      <td>[{u'api_detail_url': u'http://www.giantbomb.co...</td>\n",
       "      <td>1986-06-15 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Put on some tight red spandex pants and grab y...</td>\n",
       "      <td>Bass Avenger</td>\n",
       "      <td>[{u'api_detail_url': u'http://www.giantbomb.co...</td>\n",
       "      <td>2000-09-28 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Smackdown Vs. Raw 2007 is the third installmen...</td>\n",
       "      <td>WWE SmackDown! vs. RAW 2007</td>\n",
       "      <td>[{u'api_detail_url': u'http://www.giantbomb.co...</td>\n",
       "      <td>2006-11-14 00:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               decks  \\\n",
       "0  A top-down isometric helicopter shoot 'em up o...   \n",
       "1  Breakfree is a block-breaking game that is sim...   \n",
       "2  The Chessmaster 2000 is the chess game that be...   \n",
       "3  Put on some tight red spandex pants and grab y...   \n",
       "4  Smackdown Vs. Raw 2007 is the third installmen...   \n",
       "\n",
       "                               names  \\\n",
       "0  Desert Strike: Return to the Gulf   \n",
       "1                          Breakfree   \n",
       "2               The Chessmaster 2000   \n",
       "3                       Bass Avenger   \n",
       "4        WWE SmackDown! vs. RAW 2007   \n",
       "\n",
       "                                           platforms             releases  \n",
       "0  [{u'api_detail_url': u'http://www.giantbomb.co...  1992-02-29 00:00:00  \n",
       "1  [{u'api_detail_url': u'http://www.giantbomb.co...  1995-12-31 00:00:00  \n",
       "2  [{u'api_detail_url': u'http://www.giantbomb.co...  1986-06-15 00:00:00  \n",
       "3  [{u'api_detail_url': u'http://www.giantbomb.co...  2000-09-28 00:00:00  \n",
       "4  [{u'api_detail_url': u'http://www.giantbomb.co...  2006-11-14 00:00:00  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(game_data).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that for our project, we really will only be delving into the \"**decks**\" variable, which provides a brief summary of the game.  Next, we need to start preparing for text mining.  The first step in successful text mining is to *tokenize* the text; that is, to break it up into smaller, digestable chunks.  In this case, we will break our game descriptions into lists of single word tokens.  Additionally, we will *stem* each word, which means that we will chop off the end of words such that different permutations of the same word can be understood to be related.  Lastly, we will also remove all punctuation from our text objects.  The function we define below performs all three of these tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define a new tokenizer for clustering\n",
    "def tokenize_and_stem(text):\n",
    "    snowball = nltk.stem.snowball.SnowballStemmer('english')\n",
    "    punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    tokens = []\n",
    "    stems = []\n",
    "    text = text.lower()\n",
    "    text = punc.sub('', text)\n",
    "    tokens.append(nltk.word_tokenize(text))\n",
    "    for token in tokens:\n",
    "        for word in token:\n",
    "            stems.append(snowball.stem(word))\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of our function's output by calling it on the first game description in our data set and showing the first 10 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'a',\n",
       " u'topdown',\n",
       " u'isometr',\n",
       " u'helicopt',\n",
       " u'shoot',\n",
       " u'em',\n",
       " u'up',\n",
       " u'origin',\n",
       " u'for',\n",
       " u'the']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_and_stem(game_data['decks'][0])[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we're going to define another tokenizer.  \"*Now Rich, we already have a tokenizer, why on earth do we need another one?*\" you are wondering to yourself.  Well mysterious person reading this, we will get to that!  Have some patience!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define another tokenizer that doesn't stem so we can match stems to words later  \n",
    "def tokenize_no_stem(text):\n",
    "    punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    tokens = []\n",
    "    words = []\n",
    "    text = text.lower()\n",
    "    text = punc.sub('', text)\n",
    "    tokens.append(nltk.word_tokenize(text))\n",
    "    for token in tokens:\n",
    "        for word in token:\n",
    "            words.append(word)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next function explains the need for our second tokenizer.  Essentially, I want to have a table in which I can look up the non-stemmed version of any of my stemmed words.  The function below creates a DataFrame in which I can pass the stemmed word in as the index and find the non-stemmed version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create list of stemmed an non-stemmed words to match up\n",
    "def vocab(text):\n",
    "    totalvocab_stemmed = []\n",
    "    totalvocab_tokenized = []\n",
    "    for i in text:\n",
    "        allwords_stemmed = tokenize_and_stem(i)\n",
    "        totalvocab_stemmed.extend(allwords_stemmed)\n",
    "        allwords_tokenized = tokenize_no_stem(i)\n",
    "        totalvocab_tokenized.extend(allwords_tokenized)    \n",
    "            \n",
    "    vocab_frame = pd.DataFrame({'words': totalvocab_tokenized}, index = totalvocab_stemmed)\n",
    "    return vocab_frame\n",
    "vocab_frame = vocab(game_data['decks'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a quick snapshot of the construction of this lookup table.  This table will be useful later on in our exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topdown</th>\n",
       "      <td>topdown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>isometr</th>\n",
       "      <td>isometric</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>helicopt</th>\n",
       "      <td>helicopter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shoot</th>\n",
       "      <td>shoot</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               words\n",
       "a                  a\n",
       "topdown      topdown\n",
       "isometr    isometric\n",
       "helicopt  helicopter\n",
       "shoot          shoot"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_frame.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to define our *stop words*, which are essentially words that add no value to our end goal.  These words will be removed from our analysis.  Here we define the list \"**stop_words**\" by calling a pre-existing list of common stop words from NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop_words = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the first 10 stop words, just to give you an idea of what kind of words we don't care about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'i',\n",
       " u'me',\n",
       " u'my',\n",
       " u'myself',\n",
       " u'we',\n",
       " u'our',\n",
       " u'ours',\n",
       " u'ourselves',\n",
       " u'you',\n",
       " u'your']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to create our *TF-IDF matrix* which measures two things.  First, it counts the number of times a word appears in a given document.  Then, this count is weighted such that words that appear in many documents are penalized for being more common.  The theory being, that if a word only appears in a few documents, it is likely important in distinguishing that document.\n",
    "\n",
    "The TfidfVectorizer function below creates this matrix for us.  A few things to note:\n",
    " * We pass the function our list of stop words so that these are not included in our TF-IDF matrix.\n",
    " * The arguments \"**max_df**\" and \"**min_df**\" specify to only include words that are included in less than 30 percent of documents but at least 0.1% of documents.\n",
    " * The argument \"**use_idf**\" simply tells the vectorizer to use the inverse document frequency weighting-scheme we mentioned above.\n",
    " * Lastly, we pass in the tokenizer we defined above to transform each word entered into our matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define vectorizer parameters\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=stop_words, max_df=0.3, min_df = 0.01,\n",
    "                                 use_idf=True, tokenizer=tokenize_and_stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40533, 160)\n"
     ]
    }
   ],
   "source": [
    "#Create TFIDF Matrix\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(game_data['decks'])\n",
    "print(tfidf_matrix.shape)\n",
    "terms = tfidf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The TF-IDF matrix we've created consists of **40,430** documents (the number of total games Giant Bomb has documented in their wiki) and **161** words that meet the criteria we specified above.  Next, we use the TF-IDF matrix as the input to the KMeans clustering algorithm to create out genre clusters.  Here I've rather arbitrarily chosen 8 as the number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#KMeans Clustering\n",
    "num_clusters = 8\n",
    "km = KMeans(n_clusters = num_clusters, random_state=3)\n",
    "km.fit(tfidf_matrix)\n",
    "clusters = km.labels_.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, using that lookup table we created earlier, we can see those words in each cluster that are closest to the cluster centroid.  Here are the three closest words for each of our clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster:\n",
      "\n",
      "Cluster 0 words: rpg, action, developed,\n",
      "Cluster 1 words: puzzle, players, platforms,\n",
      "Cluster 2 words: racing, released, arcade,\n",
      "Cluster 3 words: developed, published, released,\n",
      "Cluster 4 words: novel, visuals, developed,\n",
      "Cluster 5 words: based, named, series,\n",
      "Cluster 6 words: released, players, series,\n",
      "Cluster 7 words: adventure, developed, series,\n"
     ]
    }
   ],
   "source": [
    "print(\"Top terms per cluster:\")\n",
    "print()\n",
    "#sort cluster centers by proximity to centroid\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1] \n",
    "\n",
    "for i in range(num_clusters):\n",
    "    print(\"Cluster %d words:\" % i, end='')\n",
    "    \n",
    "    for ind in order_centroids[i, :3]: \n",
    "        print(' %s' % vocab_frame.ix[terms[ind].split(' ')].values.tolist()[0][0], end=',')\n",
    "    print() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This output is a decent start.  Note there are a few clusters that make a lot of sense!  Also, a few of our clusters are already starting to show signs of looking like the genres we would expect.  For instance, *Cluster 1* seems to represent puzzle platformers and *Cluster 0* seems to represent action RPGs!  Pretty cool right?\n",
    "\n",
    "However, we have a few clusters that are completely nonsensical for what we are trying to achieve.  For example, *Cluster 3* just returns words associated with the process of making games. This doesn't help us at all.\n",
    "\n",
    "One thing we can do is add words that we don't find useful to our list of stop words.  This can be tedious and manually-intensive, but it can help us get better-defined genres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Add a bunch of stop words not relevant to a games genre\n",
    "stop_words.append('featur')\n",
    "stop_words.append('sequel')\n",
    "stop_words.append('originally')\n",
    "stop_words.append('set')\n",
    "stop_words.append('based')\n",
    "stop_words.append('franchis')\n",
    "stop_words.append('seri')\n",
    "stop_words.append('instal')\n",
    "stop_words.append('first')\n",
    "stop_words.append('one')\n",
    "stop_words.append('second')\n",
    "stop_words.append('two')\n",
    "stop_words.append('2')\n",
    "stop_words.append('third')\n",
    "stop_words.append('final')\n",
    "stop_words.append('version')\n",
    "stop_words.append('entri')\n",
    "stop_words.append('new')\n",
    "stop_words.append('develop')\n",
    "stop_words.append('publish')\n",
    "stop_words.append('releas')\n",
    "stop_words.append('pc')\n",
    "stop_words.append('nes')\n",
    "stop_words.append('playstat')\n",
    "stop_words.append('nintendo')\n",
    "stop_words.append('wii')\n",
    "stop_words.append('ds')\n",
    "stop_words.append('io')\n",
    "stop_words.append('famicom')\n",
    "stop_words.append('sega')\n",
    "stop_words.append('xbox')\n",
    "stop_words.append('live')\n",
    "stop_words.append('origin')\n",
    "stop_words.append('japan')\n",
    "stop_words.append('boy')\n",
    "stop_words.append('color')\n",
    "stop_words.append('must')\n",
    "stop_words.append('base')\n",
    "stop_words.append('name')\n",
    "stop_words.append('terms')\n",
    "stop_words.append('play')\n",
    "stop_words.append('player')\n",
    "stop_words.append('popular')\n",
    "stop_words.append('onli')\n",
    "stop_words.append('use')\n",
    "stop_words.append('control')\n",
    "stop_words.append('take')\n",
    "stop_words.append('super')\n",
    "stop_words.append('made')\n",
    "stop_words.append('creat')\n",
    "stop_words.append('exclus')\n",
    "stop_words.append('includ')\n",
    "stop_words.append('tri')\n",
    "stop_words.append('video')\n",
    "stop_words.append('studio')\n",
    "stop_words.append('like')\n",
    "stop_words.append('system')\n",
    "stop_words.append('back')\n",
    "stop_words.append('find')\n",
    "stop_words.append('help')\n",
    "stop_words.append('graphic')\n",
    "stop_words.append('gameplay')\n",
    "stop_words.append('star')\n",
    "stop_words.append('character')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's re-define our TF-IDF matrix and re-run our clustering algorithm and see the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40533, 100)\n"
     ]
    }
   ],
   "source": [
    "#Define vectorizer parameters\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=stop_words, max_df=0.3, min_df = 0.01,\n",
    "                                 use_idf=True, tokenizer=tokenize_and_stem)\n",
    "\n",
    "#Create TFIDF Matrix\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(game_data['decks'])\n",
    "print(tfidf_matrix.shape)\n",
    "terms = tfidf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster:\n",
      "\n",
      "Cluster 0 words: robots, war, battle,\n",
      "Cluster 1 words: simulation, manager, combat,\n",
      "Cluster 2 words: action, shooter, platforms,\n",
      "Cluster 3 words: fight, character, 2d,\n",
      "Cluster 4 words: world, war, adventure,\n",
      "Cluster 5 words: puzzle, platforms, adventure,\n",
      "Cluster 6 words: adventure, action, mystery,\n",
      "Cluster 7 words: strategy, turnbased, war,\n"
     ]
    }
   ],
   "source": [
    "#KMeans Clustering\n",
    "num_clusters = 8\n",
    "km = KMeans(n_clusters = num_clusters, random_state=3)\n",
    "km.fit(tfidf_matrix)\n",
    "clusters = km.labels_.tolist()\n",
    "\n",
    "print(\"Top terms per cluster:\")\n",
    "print()\n",
    "#sort cluster centers by proximity to centroid\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1] \n",
    "\n",
    "for i in range(num_clusters):\n",
    "    print(\"Cluster %d words:\" % i, end='')\n",
    "    \n",
    "    for ind in order_centroids[i, :3]: \n",
    "        print(' %s' % vocab_frame.ix[terms[ind].split(' ')].values.tolist()[0][0], end=',')\n",
    "    print() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks way better--our new genres make a lot more sense.  Let's take a look at some of our favorite titles to see how we did and where our algorithm falls short."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Creates a Series using the Game Name as the Index\n",
    "game_genres = pd.Series(clusters, index=game_data['names'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the Series defined above, we can now look up games we know and love using their name, returning their output cluster as defined by our algorithm.  Let's look at the first **Halo**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game_genres['Halo: Combat Evolved']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Cluster 2 - Action, Shooter, Platforms*** - Not bad!  Halo is definitely a shooter, although  it's definitely not a platformer.  I would agree with the algorithm that *Cluster 2* seems to make the most sense.  Next let's look at another one of my favorites, **World of Warcraft**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game_genres['World of Warcraft']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Cluster 4 - World, War, Adventure*** - Eh, this isn't great, and probably demonstrates one of the flaws of our algorithm.  WoW is a textbook MMO-RPG but we don't really have a cluster that seems to fit those types of games.  Here it looks like the name itself (i.e. **World** of **War**craft) was the deciding factor in terms of its cluster assignment, which isn't exactly what we're looking for.  Let's see if our algorithm can identify fighting games--next up, **Mortal Kombat X**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game_genres['Mortal Kombat X']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Cluster 3 - Fight, Character, 2d*** - This checks out!  What about the new Jonathan Blow open-world puzzle game that has everybody all hot and bothered: **The Witness**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game_genres['The Witness']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Cluster 5 - Puzzle, Platforms, Adventure*** - Nailed this one also.  Cool!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Summary\n",
    "As you can see, using text-mining and document clustering techniques we can fairly effectively assign games into genres that accurately define them.  I could go on and show you dozens more examples of both games I feel we've classified correctly as well as games in which I don't think are quite right, but instead mysterious reader I implore you to copy the code and see where your favorite game ends up!  \n",
    "\n",
    "Thanks for reading and please feel free to reach me at **richard_conboy@ncsu.edu** or **845-264-2406** with any questions!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
